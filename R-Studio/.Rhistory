library(ggplot2)
library(GGally)
ggpairs(iris_data)
# q9
# d)
apply(iris_data[,3:4],1,mean)
# b)
sapply(iris_data[,3:4],mean)
# c)
lapply(iris_data[,3:4],mean)
# q10
sum(is.na(iris_data))
library(readr)
iris_csv_iris <- read_csv("Week-7/iris.csv - iris.csv")
View(iris_csv_iris)
rm(list = ls())
iris_data <- read.csv("Week-7/iris.csv - iris.csv",header = T,row.names = "ID")
rm(list = ls())
iris_data <- read.csv("Week-7/iris.csv - iris.csv",header = T,row.names = "ID")
View(iris_data)
set.seed(123)
sample = split(iris_data$Species,0.7)
View(sample)
sample[["0.7"]]
sample = sample.split(iris_data$Species,0.7)
sample = sample.split(iris_data$Species,SplitRatio= 0.7)
library(caTools)
set.seed(123)
sample = sample.split(iris_data $ Species,SplitRatio= 0.7)
train_data = subset(iris_data $Species , split.data.frame = T)
train_data = subset(iris_data , split.data.frame = T)
test_data = subset(iris_data , split.data.frame = F)
View(train_data)
train_data = subset(iris_data , split.data = T)
test_data = subset(iris_data , split.data = F)
sample
train_data = subset(iris_data , sample = T)
test_data = subset(iris_data , sample = F)
train_data = subset(iris_data , sample == T)
test_data = subset(iris_data , sample == F)
View(train_data)
rm(list = ls())
iris_data <- read.csv("Week-7/iris.csv - iris.csv",header = T,row.names = "ID")
library(caTools)
set.seed(123)
sample = sample.split(iris_data $ Species,SplitRatio= 0.7)
sample
train_data = subset(iris_data , sample == T)
test_data = subset(iris_data , sample == F)
str(train_data)
summary(train_data)
iris_data <- read.csv("Week-7/iris.csv - iris.csv",header = T,row.names = "ID",stringsAsFactors = T)
str(train_data)
summary(train_data)
rm(list = ls())
iris_data <- read.csv("Week-7/iris.csv - iris.csv",header = T,row.names = "ID",stringsAsFactors = T)
library(caTools)
set.seed(123)
sample = sample.split(iris_data $ Species,SplitRatio= 0.7)
sample
train_data = subset(iris_data , sample == T)
test_data = subset(iris_data , sample == F)
str(train_data)
summary(train_data)
train=glm(formula = train_data$Species~. , family = "binomial")
train=glm(formula = Species~. , family = "binomial" , data = train_data)
summary(train)
# or
train2 = glm (formula = Species ~ SepalLength+SepalWidth +PetalLength+ PetalWidth ,data = train_data)
# or
train2 = glm (formula = Species ~ SepalLength+SepalWidth +PetalLength+ PetalWidth,  family = "binomial"  ,data = train_data)
summary(train)
rm(list = ls())
iris_data <- read.csv("Week-7/iris.csv - iris.csv",header = T,row.names = "ID",stringsAsFactors = T)
library(caTools)
set.seed(123)
sample = sample.split(iris_data $ Species,SplitRatio= 0.7)
sample
train_data = subset(iris_data , sample == T)
test_data = subset(iris_data , sample == F)
str(train_data)
summary(train_data)
train1=glm(formula = Species~. , family = "binomial" , data = train_data)
# or
train2 = glm (formula = Species ~ SepalLength+SepalWidth +PetalLength+ PetalWidth,  family = "binomial"  ,data = train_data)
summary(train2)
# or
train2 = glm (formula = Species ~ SepalLength+SepalWidth +PetalLength+ PetalWidth,data = train_data)
rm(list = ls())
iris_data <- read.csv("Week-7/iris.csv - iris.csv",header = T,row.names = "ID",stringsAsFactors = T)
library(caTools)
set.seed(123)
sample = sample.split(iris_data $ Species,SplitRatio= 0.7)
sample
train_data = subset(iris_data , sample == T)
test_data = subset(iris_data , sample == F)
str(train_data)
summary(train_data)
train1=glm(formula = Species~. , family = "binomial" , data = train_data)
# or
# train2 = glm (formula = Species ~ SepalLength+SepalWidth +PetalLength+ PetalWidth,data = train_data)
summary(train2)
# or
# train2 = glm (formula = Species ~ SepalLength+SepalWidth +PetalLength+ PetalWidth,data = train_data)
summary(train1)
rm(list = ls())
iris_data <- read.csv("Week-7/iris.csv - iris.csv",header = T,row.names = "ID",stringsAsFactors = T)
# library(caTools)
# set.seed(123)
# sample = sample.split(iris_data $ Species,SplitRatio= 0.7)
# sample
# train_data = subset(iris_data , sample == T)
# test_data = subset(iris_data , sample == F)
str(iris_data)
summary(iris_data)
formula_apply=glm(formula = Species~. , family = "binomial" , data = iris_data)
# or
# train2 = glm (formula = Species ~ SepalLength+SepalWidth +PetalLength+ PetalWidth,data = train_data)
summary(train1)
# or
# train2 = glm (formula = Species ~ SepalLength+SepalWidth +PetalLength+ PetalWidth,data = train_data)
summary(formula_apply)
formula_apply=glm(formula = Species~., data = iris_data)
rm(list = ls())
str(iris_data)
rm(list = ls())
iris_data <- read.csv("Week-7/iris.csv - iris.csv",header = T,row.names = "ID")
str(iris_data)
rm(list = ls())
iris_data <- read.csv("Week-7/iris.csv - iris.csv",header = T,row.names = "ID")
iris_data = as.factor(iris_data$Species)
str(iris_data)
iris_data$Species = as.factor(iris_data$Species)
rm(list = ls())
iris_data <- read.csv("Week-7/iris.csv - iris.csv",header = T,row.names = "ID")
iris_data $ Species = as.factor(iris_data$Species)
str(iris_data)
summary(iris_data)
iris_data = iris_data[1:100,]
View(iris_data)
str(iris_data)
summary(iris_data)
library(ggplot2)
library(GGally)
ggpairs(iris_data)
rm(list=ls())
source("D:/Visual Studio Code/GIT-1/R-Studio/Week-7/Logistic-Regrssion.R", echo=TRUE)
rm(list = ls())
data=read.csv("Week-8/USArrests.csv",header = T,row.names = "States")
df=scale(data)
set.seed(123)
fit=kmeans(df,centers = 4,nstart = 20)
fit
print(fit$withinss)
a=matrix(c(1,2,3,0,0,1),nrow = 3,ncol = 2)
rm(list = ls())
a=matrix(c(1,2,3,0,0,1),nrow = 3,ncol = 2)
b=matrix(c(1,2,5),nrow=3,byrow = FALSE)
# op=inv(t(a)%*%a) %*% (t(a) %*% b)
# print(op)
print("-----------------------------------")
library(MASS)
c=ginv(a)%*%b
View(c)
View(b)
View(a)
ginv(a)
rm(list = ls())
rm(list = ls())
#2
A=matrix(c(1,2,3,2,3,6,4,5,9),3,3,byrow = T)
rank=Rank(A)
rm(list = ls())
#2
A=matrix(c(1,2,3,2,3,6,4,5,9),3,3,byrow = T)
library(pracma)
rank=Rank(A)
install.packages("pracma")
rm(list = ls())
#2
A=matrix(c(1,2,3,2,3,6,4,5,9),3,3,byrow = T)
library(pracma)
rank=Rank(A)
print(rank)
View(A)
#------------------
#eg1
A=matrix(c(1,2,4,3),2,2)
print(eigen(A))
#4
A=matrix(c(1,2,4,3),2,2)
print(eigen(A))
#--------------------
#5
A=matrix(c(-1,2,2),3,1,byrow = T)
B=t(A)
C=A%*%B
print(C)
print(eigen(C))
#-------------------------
q=2^3
print(q)
#----------------------------
#6
A=matrix(c(1,2,4,3),2,2)
# print(det(inv(A)))
# print(det(A)^-1)
B=matrix(c(1,0,0,1),2,2)
print(det(A%*%B))
print(det(A)*det(B))
#------------------------
#7 part1
A=matrix(c(2,1,3,-1,1,-3,3,-2,8),3,3,byrow = T)
print(det(A))
#8
A=matrix(c(1,2,3,2,5,7,2,4,6),3,3,byrow = T)
print(Rank(A))
print(rank(A))
#9
v1=matrix(c(1,-1,-2),3,1)
v2=matrix(c(2,0,1),1,3)
Z=v1%*%v2
print(det(Z))
rm(list = ls())
A=matrix(c(0.36,0.48,0,0.48,0.64,0,0,0,2),nrow = 3,byrow = T)
# colnames(A)=c('a','b','c')
# rownames(A)=c('d','e','f')
lamda=matrix(c(0,1,2),nrow = 3)
i=matrix(c(1,0,0,0,1,0,0,0,1),nrow = 3,byrow = TRUE)
ei=i%*%lamda
v1=A[,1]-ei
v2=A[,2]-ei
v3=A[,3]-ei
print(v1)
print(v2)
print(v3)
v=c("green","apple")
c=2
repeat{
print(v)
c=c+1
if(c>5){
break
}
}
a=matrix(1:42,nrow = 6,ncol = 7,byrow = TRUE)
b=a[-2,]
print(a)
print(b)
name=c("city1","city2","city3")
weather=c("sunny","cloudy","rainy")
city=data.frame(name,weather)
View(city)
rm(list = ls())
name=c("city1","city2","city3")
weather=c("sunny","cloudy","rainy")
city=data.frame(name,weather)
city$weather[city$name=="city3"]="snowy"
View(city)
print(city)
cityweather=list("city1","city2","city3",c("sunny","cloudy","rainy"))
print(cityweather[[4]][2])
View(cityweather)
cityweather[[4]]
cityweather=list("city1" , c("sunny","cloudy","rainy"),"city2","city3")
print(cityweather[[4]][2])
View(cityweather)
rm(list = ls())
rm(list = ls())
vec1=c(1,2,3)
vec2=c("R","Scilab","Java")
vec3=c("For DataScience","For Don't Know","For Mandatory")
df=data.frame(vec1,vec2,vec3)
print(df)
View(df)
print(df[1:2,]) #To Access 1st and 2nd row
print(df[,1:2]) #To Access 1st and 2nd coloumn
df[[2]][2]="Python"
df[[3]][2]="For DataScience"
print(df)
name=c("city1","city2","city3")
weather=c("sunny","cloudy","rainy")
city=data.frame(name,weather)
city$weather[city$name=="city3"]="snowy"
print(city)
cityweather=list("city1","city2","city3",c("sunny","cloudy","rainy"))
print(cityweather[[4]][2])
rm(list = ls())
cityweather=list("city1","city2","city3",c("sunny","cloudy","rainy"))
print(cityweather[[4]][2])
print(cityweather)
name=c("city1","city2","city3")
weather=c("sunny","cloudy","rainy")
city=data.frame(name,weather)
print(city)
city[2]
city[[2]]
city[[2]][1]
city[,2]
city[2,]
city[1][[2]]
rm(list = ls())
vec1=c(1,2,3)
vec2=c("R","Scilab","Java")
vec3=c("For DataScience","For Don't Know","For Mandatory")
df=data.frame(vec1,vec2,vec3)
print(df)
print("-----------------------------")
print(df[1:2,]) #To Access 1st and 2nd row
print(df[,1:2]) #To Access 1st and 2nd coloumn
df[[2]][2]="Python"
df[[3]][2]="For DataScience"
rm(list = ls())
vec1=c(1,2,3)
vec2=c("R","Scilab","Java")
vec3=c("For DataScience","For Don't Know","For Mandatory")
df=data.frame(vec1,vec2,vec3)
print(df)
print("-----------------------------")
print(df[1:2,]) #To Access 1st and 2nd row
print(df[,1:2]) #To Access 1st and 2nd coloumn
df[[2]][2]="Python"
df[[3]][2]="DataScience"
print(df)
vec1=c(1,2,3)
vec2=c("R","Scilab","Java")
vec3=c("For DataScience","For Don't Know","For Mandatory")
df=data.frame(vec1,vec2,vec3)
print(df)
print("-----------------------------")
print(df[1:2,]) #To Access 1st and 2nd row
print(df[,1:2]) #To Access 1st and 2nd coloumn
df[[2]][2]="Python"
df[[3]][2]="others"
print(df)
rm(list = ls())
library(reshape2)
library(dplyr)
pd=data.frame("Name"=c("senthil","senthil","sam","sam"),"month"=c("jan","feb","jan","feb"),"bs"=c(98,96,90,89),"bp"=c(140,143,148,145))
pd2=data.frame("Name"=c("senthil","Ramesh","sam"),"Department"=c("PSE","data analyst","PSE"))
pd_new=full_join(pd,pd2,by="Name")
View(pd2)
View(pd_new)
a=matrix(c(-1/4,0,0,-1/3),2,2,byrow = T)
t(a)
View(a)
b=matrix(c(1,2,3,4),2,2,byrow = T)
t(b)
View(b)
A=matrix(c(8,-4,-4,4),2,2,byrow = T)
print(eigen(A))
source("D:/Visual Studio Code/GIT-1/R-Pro/Machine-Learning-Stats-Quest/ROC&AUC-Graph.R", echo=TRUE)
install.packages("pROC")
library(pROC)
install.packages("randomForest")
library(randomForest)
source("D:/Visual Studio Code/GIT-1/R-Pro/Machine-Learning-Stats-Quest/ROC&AUC-Graph.R", echo=TRUE)
rm(list = ls())
rm(list = ls())
# Here's the data
mouse.data <- data.frame(
size = c(1.4, 2.6, 1.0, 3.7, 5.5, 3.2, 3.0, 4.9, 6.3),
weight = c(0.9, 1.8, 2.4, 3.5, 3.9, 4.4, 5.1, 5.6, 6.3),
tail = c(0.7, 1.3, 0.7, 2.0, 3.6, 3.0, 2.9, 3.9, 4.0))
View(mouse.data)
mouse.data
## STEP 1: Draw a graph of the data to make sure the relationship make sense
plot(mouse.data$weight, mouse.data$size, pch=16, cex=2)
## STEP 1: Draw a graph of the data to make sure the relationship make sense
plot(mouse.data$weight, mouse.data$size, pch=19, cex=2)
## STEP 1: Draw a graph of the data to make sure the relationship make sense
plot(mouse.data$weight, mouse.data$size, pch=19, cex=1)
rm(list = ls())
# Here's the data
mouse.data <- data.frame(
size = c(1.4, 2.6, 1.0, 3.7, 5.5, 3.2, 3.0, 4.9, 6.3),
weight = c(0.9, 1.8, 2.4, 3.5, 3.9, 4.4, 5.1, 5.6, 6.3),
tail = c(0.7, 1.3, 0.7, 2.0, 3.6, 3.0, 2.9, 3.9, 4.0))
mouse.data
## STEP 1: Draw a graph of the data to make sure the relationship make sense
plot(mouse.data$weight, mouse.data$size, pch=19, cex=1)
## STEP 2: Do the regression
simple.regression <- lm(size ~ weight, data=mouse.data)
View(simple.regression)
View(mouse.data)
## STEP 3: Look at the R^2, F-value and p-value
summary(simple.regression)
abline(simple.regression, lwd=5, col="red")
abline(simple.regression, lwd=2, col="red")
abline(simple.regression, lwd=5, col="red")
abline(simple.regression, lwd=2, col="red")
rm(list = ls())
# Here's the data
mouse.data <- data.frame(
size = c(1.4, 2.6, 1.0, 3.7, 5.5, 3.2, 3.0, 4.9, 6.3),
weight = c(0.9, 1.8, 2.4, 3.5, 3.9, 4.4, 5.1, 5.6, 6.3),
tail = c(0.7, 1.3, 0.7, 2.0, 3.6, 3.0, 2.9, 3.9, 4.0))
mouse.data
#######################################################
##
## Let's start by reviewing simple regression by
## modeling mouse size with mouse weight.
##
#######################################################
## STEP 1: Draw a graph of the data to make sure the relationship make sense
plot(mouse.data$weight, mouse.data$size, pch=19, cex=1)
## STEP 2: Do the regression
simple.regression <- lm(size ~ weight, data=mouse.data)
## STEP 3: Look at the R^2, F-value and p-value
summary(simple.regression)
abline(simple.regression, lwd=2, col="red")
ss.mean <- sum((mouse.data$size - mean(mouse.data$size))^2)
ss.simple <- sum(simple.regression$residuals^2)
(ss.mean - ss.simple) / ss.mean # this is the R^2 value
## now let's verify the our formula for F is correct...
f.simple <- ((ss.mean - ss.simple) / (2 - 1)) /
(ss.simple / (nrow(mouse.data) - 2))
f.simple # this is the F-value
## Now let's draw a figure that shows how to calculate the p-value from the
## F-value
##
## First, draw the correct f-distribution curve with df1=1 and df2=7
x <- seq(from=0, to=15, by=0.1)
y <- df(x, df1=1, df2=7)
plot(x, y, type="l")
## now draw a verticle line where our F-value, f.simple, is.
abline(v=f.simple, col="red")
## color the graph on the left side of the line blue
x.zero.to.line <- seq(from=0, to=f.simple, by=0.1)
y.zero.to.line <- df(x.zero.to.line, df1=1, df2=7)
polygon(x=c(x.zero.to.line, 0), y=c(y.zero.to.line, 0), col="blue")
x.line.to.20 <- seq(from=f.simple, to=20, by=0.1)
y.line.to.20 <- df(x.line.to.20, df1=1, df2=7)
polygon(x=c(x.line.to.20, f.simple), y=c(y.line.to.20, 0), col="red")
pf(f.simple, df1=1, df2=7) ## the area under the curve that is blue
1-pf(f.simple, df1=1, df2=7) ## the area under the curve that is red
## lastly, let's compare this p-value to the one from the
## original regression
summary(simple.regression)
## STEP 1: Draw a graph of the data to make sure the relationship make sense
## This graph is more complex because it shows the relationships between all
## of the columns in "mouse.data".
plot(mouse.data)
## STEP 2: Do the regression
multiple.regression <- lm(size ~ weight + tail, data=mouse.data)
## STEP 3: Look at the R^2, F-value and p-value
summary(multiple.regression)
View(multiple.regression)
## again, we can verify that our R^2 value is what we think it is
ss.multiple <- sum(multiple.regression$residuals^2)
(ss.mean - ss.multiple) / ss.mean
## we can also verify that the F-value is what we think it is
f.multiple <- ((ss.mean - ss.multiple) / (3 - 1)) /
(ss.multiple / (nrow(mouse.data) - 3))
f.multiple
## Again let's draw a figure that shows how to calculate the p-value from the
## F-value
##
## First, draw the correct f-distribution curve with df1=2 and df2=6
x <- seq(from=0, to=20, by=0.1)
y <- df(x, df1=2, df2=6)
plot(x, y, type="l")
## now draw a verticle line where our f.value is for this test
abline(v=f.multiple, col="red")
## color the graph on the left side of the line blue
x.zero.to.line <- seq(from=0, to=f.multiple, by=0.1)
y.zero.to.line <- df(x.zero.to.line, df1=2, df2=6)
polygon(x=c(x.zero.to.line, 0), y=c(y.zero.to.line, 0), col="blue")
## color the graph on the right side of the line red
x.line.to.20 <- seq(from=f.multiple, to=20, by=0.1)
y.line.to.20 <- df(x.line.to.20, df1=2, df2=6)
polygon(x=c(x.line.to.20, f.multiple), y=c(y.line.to.20, 0), col="red")
pf(f.multiple, df1=2, df2=6) ## the area under the curve that is blue
1-pf(f.multiple, df1=2, df2=6) ## the area under the curve that is red
## lastly, let's compare this p-value to the one from the
## original regression
summary(multiple.regression)
f.simple.v.multiple <- ((ss.simple - ss.multiple) / (3-2)) /
(ss.multiple / (nrow(mouse.data) - 3))
1-pf(f.simple.v.multiple, df1=1, df2=6)
## Notice that this value is the same as the p-value next to the term for
## for "tail" in the summary of multiple regression:
summary(multiple.regression)
rm(list = ls())
rm(list = ls())
library(ggplot2)
library(cowplot)
rm(list=ls())
crashTest_1 <- read.csv("Week-7/crashTest_1.csv",row.names=1)
crashTest_1_TEST <- read.csv("Week-7/crashTest_1_TEST.csv",row.names=1)
crashTest_1$CarType=as.factor(crashTest_1$CarType) #very very important because to convert suv and hatchback to binomial variables ie 0 and 1
library(tidyverse)
# crashTest_1=crashTest_1 %>% mutate(recode(crashTest_1$CarType,"SUV"="0" , "Hatchback"="1"))
# factor(crashTest_1$CarType , c(1,2))
# structure of data
str(crashTest_1)
# summary
summary(crashTest_1)
# model building
logisfit=glm(formula=CarType~. ,family = 'binomial' , data = crashTest_1)
summary(logisfit)
logistrain=predict(logisfit,type='response')
plot(logistrain)
tapply(logistrain,crashTest_1$CarType,mean)
logispred=predict(logisfit,newdata = crashTest_1_TEST,type = 'response')
plot(logispred)
crashTest_1_TEST[logispred<=0.5,"logispred"]="Hatchback"
crashTest_1_TEST[logispred>0.5,"logispred"]="SUV"
library(caret)
confusionMatrix(table(crashTest_1_TEST[,7],crashTest_1_TEST[,6]))
rm(list=ls())
install.packages("glmnet")
