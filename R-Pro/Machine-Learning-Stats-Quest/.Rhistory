rm(list = ls())
## install "glmnet" package with: install.packages("glmnet")
library(glmnet)  # Package to fit ridge/lasso/elastic net models
set.seed(42)  # Set seed for reproducibility
n <- 1000  # Number of observations
p <- 5000  # Number of predictors included in model
real_p <- 15  # Number of true predictors
## Generate the data
x <- matrix(rnorm(n*p), nrow=n, ncol=p)
y <- apply(x[,1:real_p], 1, sum) + rnorm(n)
View(x)
gc()
rm(list = ls())
rm(list = ls())
## install "glmnet" package with: install.packages("glmnet")
library(glmnet)  # Package to fit ridge/lasso/elastic net models
set.seed(42)  # Set seed for reproducibility
n <- 1000  # Number of observations
p <- 5000  # Number of predictors included in model
real_p <- 15  # Number of true predictors
## Generate the data
x <- matrix(rnorm(n*p), nrow=n, ncol=p)
y <- apply(x[,1:real_p], 1, sum) + rnorm(n)
View(x)
y
## Split data into training and testing datasets.
## 2/3rds of the data will be used for Training and 1/3 of the
## data will be used for Testing.
train_rows <- sample(1:n, .66*n)
train_rows
x.train <- x[train_rows, ]
x.test <- x[-train_rows, ]
y.train <- y[train_rows]
y.train
y.test <- y[-train_rows]
################################
##
## alpha = 0, Ridge Regression
##
################################
alpha0.fit <- cv.glmnet(x.train, y.train, type.measure="mse",
alpha=0, family="gaussian")
View(alpha0.fit)
## now let's run the Testing dataset on the model created for
## alpha = 0 (i.e. Ridge Regression).
alpha0.predicted <-
predict(alpha0.fit, s=alpha0.fit$lambda.1se, newx=x.test)
View(alpha0.predicted)
## Lastly, let's calculate the Mean Squared Error (MSE) for the model
## created for alpha = 0.
## The MSE is the mean of the sum of the squared difference between
## the predicted 'y' values and the true 'y' values in the
## Testing dataset...
mean((y.test - alpha0.predicted)^2)
################################
##
## alpha = 1, Lasso Regression
##
################################
alpha1.fit <- cv.glmnet(x.train, y.train, type.measure="mse",
alpha=1, family="gaussian")
alpha1.predicted <-
predict(alpha1.fit, s=alpha1.fit$lambda.1se, newx=x.test)
mean((y.test - alpha1.predicted)^2)
################################
##
## alpha = 0.5, a 50/50 mixture of Ridge and Lasso Regression
##
################################
alpha0.5.fit <- cv.glmnet(x.train, y.train, type.measure="mse",
alpha=0.5, family="gaussian")
alpha0.5.predicted <-
predict(alpha0.5.fit, s=alpha0.5.fit$lambda.1se, newx=x.test)
mean((y.test - alpha0.5.predicted)^2)
list.of.fits <- list()
for (i in 0:10) {
## Here's what's going on in this loop...
## We are testing alpha = i/10. This means we are testing
## alpha = 0/10 = 0 on the first iteration, alpha = 1/10 = 0.1 on
## the second iteration etc.
## First, make a variable name that we can use later to refer
## to the model optimized for a specific alpha.
## For example, when alpha = 0, we will be able to refer to
## that model with the variable name "alpha0".
fit.name <- paste0("alpha", i/10)
## Now fit a model (i.e. optimize lambda) and store it in a list that
## uses the variable name we just created as the reference.
list.of.fits[[fit.name]] <-
cv.glmnet(x.train, y.train, type.measure="mse", alpha=i/10,
family="gaussian")
}
## Now we see which alpha (0, 0.1, ... , 0.9, 1) does the best job
## predicting the values in the Testing dataset.
results <- data.frame()
for (i in 0:10) {
fit.name <- paste0("alpha", i/10)
## Use each model to predict 'y' given the Testing dataset
predicted <-
predict(list.of.fits[[fit.name]],
s=list.of.fits[[fit.name]]$lambda.1se, newx=x.test)
## Calculate the Mean Squared Error...
mse <- mean((y.test - predicted)^2)
## Store the results
temp <- data.frame(alpha=i/10, mse=mse, fit.name=fit.name)
results <- rbind(results, temp)
}
## View the results
results
rm(list = ls())
## In this example, the data is in a matrix called
## data.matrix
## columns are individual samples (i.e. cells)
## rows are measurements taken for all the samples (i.e. genes)
## Just for the sake of the example, here's some made up data...
data.matrix <- matrix(nrow=100, ncol=10)
View(data.matrix)
colnames(data.matrix) <- c(
paste("wt", 1:5, sep=""),
paste("ko", 1:5, sep=""))
rownames(data.matrix) <- paste("gene", 1:100, sep="")
for (i in 1:100) {
wt.values <- rpois(5, lambda=sample(x=10:1000, size=1))
ko.values <- rpois(5, lambda=sample(x=10:1000, size=1))
data.matrix[i,] <- c(wt.values, ko.values)
}
head(data.matrix)
dim(data.matrix)
pca <- prcomp(t(data.matrix), scale=TRUE)
View(pca)
## plot pc1 and pc2
plot(pca$x[,1], pca$x[,2])
pca[["sdev"]]
## make a scree plot
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per <- round(pca.var/sum(pca.var)*100, 3)
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
summary(pca)
barplot(pca.var.per, main="Scree Plot", xlab="Principal Component", ylab="Percent Variation")
## now make a fancy looking plot that shows the PCs and the variation:
library(ggplot2)
pca.data <- data.frame(Sample=rownames(pca$x),
X=pca$x[,1],
Y=pca$x[,2])
pca.data
ggplot(data=pca.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
xlab(paste("PC1 - ", pca.var.per[1], "%", sep="")) +
ylab(paste("PC2 - ", pca.var.per[2], "%", sep="")) +
theme_bw() +
ggtitle("My PCA Graph")
ggplot(data=pca.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
xlab(paste("PC1 - ", pca.var.per[1], "%", sep="+")) +
ylab(paste("PC2 - ", pca.var.per[2], "%", sep="+")) +
theme_bw() +
ggtitle("My PCA Graph")
ggplot(data=pca.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
xlab(paste("PC1 - ", pca.var.per[1], "%")) +
ylab(paste("PC2 - ", pca.var.per[2], "%")) +
theme_bw() +
ggtitle("My PCA Graph")
## get the name of the top 10 measurements (genes) that contribute
## most to pc1.
loading_scores <- pca$rotation[,1]
gene_scores <- abs(loading_scores) ## get the magnitudes
gene_score_ranked <- sort(gene_scores, decreasing=TRUE)
top_10_genes <- names(gene_score_ranked[1:10])
top_10_genes ## show the names of the top 10 genes
pca$rotation[top_10_genes,1] ## show the scores (and +/- sign)
View(pca.data)
svd.stuff <- svd(scale(t(data.matrix), center=TRUE))
## calculate the PCs
svd.data <- data.frame(Sample=colnames(data.matrix),
X=(svd.stuff$u[,1] * svd.stuff$d[1]),
Y=(svd.stuff$u[,2] * svd.stuff$d[2]))
svd.data
## alternatively, we could compute the PCs with the eigen vectors and the
## original data
svd.pcs <- t(t(svd.stuff$v) %*% t(scale(t(data.matrix), center=TRUE)))
svd.pcs[,1:2] ## the first to principal components
svd.df <- ncol(data.matrix) - 1
svd.var <- svd.stuff$d^2 / svd.df
svd.var.per <- round(svd.var/sum(svd.var)*100, 1)
ggplot(data=svd.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
xlab(paste("PC1 - ", svd.var.per[1], "%", sep="")) +
ylab(paste("PC2 - ", svd.var.per[2], "%", sep="")) +
theme_bw() +
ggtitle("svd(scale(t(data.matrix), center=TRUE)")
############################################
##
## Now let's do the same thing with eigen()
##
## eigen() returns two things...
## vectors = eigen vectors (vectors of loading scores)
##           NOTE: pcs = sum(loading scores * values for sample)
## values = eigen values
##
############################################
cov.mat <- cov(scale(t(data.matrix), center=TRUE))
dim(cov.mat)
View(cov.mat)
## since the covariance matrix is symmetric, we can tell eigen() to just
## work on the lower triangle with "symmetric=TRUE"
eigen.stuff <- eigen(cov.mat, symmetric=TRUE)
dim(eigen.stuff$vectors)
head(eigen.stuff$vectors[,1:2])
eigen.pcs <- t(t(eigen.stuff$vectors) %*% t(scale(t(data.matrix), center=TRUE)))
eigen.pcs[,1:2]
eigen.data <- data.frame(Sample=rownames(eigen.pcs),
X=(-1 * eigen.pcs[,1]), ## eigen() flips the X-axis in this case, so we flip it back
Y=eigen.pcs[,2]) ## X axis will be PC1, Y axis will be PC2
eigen.data
eigen.var.per <- round(eigen.stuff$values/sum(eigen.stuff$values)*100, 1)
ggplot(data=eigen.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
xlab(paste("PC1 - ", eigen.var.per[1], "%", sep="")) +
ylab(paste("PC2 - ", eigen.var.per[2], "%", sep="")) +
theme_bw() +
ggtitle("eigen on cov(t(data.matrix))")
gc()
data(iris)
head(iris)
?prcomp
View(iris)
myPr <- prcomp(iris[, -5])
myPr <- prcomp(iris[, -5], scale = TRUE)
View(myPr)
plot(iris$Sepal.Length, iris$Sepal.Width)
plot(scale(iris$Sepal.Length), scale(iris$Sepal.Width))
plot((iris$Sepal.Length - mean(iris$Sepal.Length)) / sd(iris$Sepal.Length))
myPr
summary(myPr)
plot(myPr, type = "l")
biplot(myPr)
biplot(myPr, scale = 0)
str(myPr)
myPr$x
iris2 <- cbind(iris, myPr$x)
View(iris2)
library(ggplot2)
ggplot(iris2, aes(PC1, PC2, col = Species, fill = Species)) +
stat_ellipse(geom = "polygon", col = "black", alpha = 0.5) +
geom_point(shape = 21, col = "black")
cor(iris[, -5], iris2[, 6:9])
